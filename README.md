# gelinkt-notuleren-consumer

Configurable consumer to sync data from external sources based on diff files generated by a producer. An example
producer can be found [here](http://github.com/lblod/mandatendatabank-mandatarissen-producer).

At regular intervals the consumer checks for new diff files and ingests the data found within. The data is ingested
in the appropriate graphs according to the authorization rules.

It does two things:
- Initial sync by getting dump files to ingest. Happens on service startup, only once, and is mandatory
- Delta sync at regular intervals where the consumer checks for new diff files and ingests the data
  found within

## Tutorials

### Add the service to a stack

1) Add the service to your `docker-compose.yml`, e.g. for the syncing of mandatarissen.

    ```yaml
    consumer:
      image: lblod/gelinkt-notuleren-consumer
      environment:
        SERVICE_NAME: 'your-custom-consumer-identifier' # replace with the desired consumer identifier
        SYNC_BASE_URL: 'http://base-sync-url # replace with link the application hosting the producer server
        SYNC_FILES_PATH: '/sync/files'
        SYNC_DATASET_SUBJECT: "http://data.lblod.info/datasets/delta-producer/dumps/MandatarissenCacheGraphDump"
        INITIAL_SYNC_JOB_OPERATION: "http://redpencil.data.gift/id/jobs/concept/JobOperation/deltas/consumer/initialSync/mandatarissen"
        JOB_CREATOR_URI: "http://data.lblod.info/services/id/mandatarissen-consumer-gelinkt-notuleren"
      volumes:
        - ./config/consumer/:/config/ # replace with path to types configuration
    ```

2) Change the `SERVICE_NAME` to the desired consumer identifier.
    - Is important as it is used to ensure persistence.

3) Change the `SYNC_BASE_URL` to the application hosting the producer server.

4) Create a `config/consumer/types.js` configuration file. It will define an array off desired types to be ingested by
   the consumer:

    ```json
    [
      {
        "type": "http://www.w3.org/ns/person#Person"
      },
      {
        "type": "http://www.w3.org/ns/person#Person",
        "pathToOrg": [
           "http://www.w3.org/ns/org#holds",
           "^http://www.w3.org/ns/org#hasPost"
        ]
      }
   ]
   ```
   each type is represented by a simple object definition:

    - `"type"` : the full URI of a type we would like to ingest
    - `"pathToOrg"` : **OPTIONAL** array of URI's that construct a path to a linked organization. Used to constuct the
      organization graph to insert the triples in.

### Automate the scheduling of sync-tasks

to achieve this we can simple add a `INGEST_INTERVAL` env. variable

```yaml
 consumer:
   image: lblod/gelinkt-notuleren-consumer
   environment:
     INGEST_INTERVAL: 60000 # each minute
```

## Configuration

The following environment variables are required:

- `SERVICE_NAME`: consumer identifier. important as it is used to ensure persistence. The identifier should be unique within the project. [REQUIRED]
- `SYNC_DATASET_SUBJECT`: subject used when fetching the dataset [REQUIRED]
- `JOB_CREATOR_URI`: URL of the creator of the sync jobs [REQUIRED]
- `INITIAL_SYNC_JOB_OPERATION`: Job operation of the sync job, used to describe the created jobs [REQUIRED]
- `INGEST_GRAPH`: graph in which all insert changesets are ingested before these are moved to the organisation graphs.
The following environment variables are optional:

- `SYNC_BASE_URL`: base URL of the stack hosting the producer API
- `SYNC_FILES_PATH (default: /sync/files)`: relative path to the endpoint to retrieve names of the diff files from
- `DOWNLOAD_FILES_PATH (default: /files/:id/download)`: relative path to the endpoint to download a diff file
  from. `:id` will be replaced with the uuid of the file.
- `CRON_PATTERN_DELTA_SYNC (default: 0 * * * * *)`: cron pattern at which the consumer needs to sync data automatically.
- `START_FROM_DELTA_TIMESTAMP (ISO datetime, default: now)`: timestamp to start sync data from (e.g. "2020-07-05T13:57:
  36.344Z") Only makes sense when initial ingest hasn't run.
- `BATCH_SIZE_FOR_GRAPH_MOVE`: moving data can be expensive, you can control the size of the batches to move around to organisation graphs
- `DISABLE_INITIAL_SYNC (default: false)`: flag to disable initial sync
- `DISABLE_INITIAL_SYNC_FIRST_INGEST (default: false)`: mainly for debugging purposes, you can skip step one of the ingestion.
- `DISABLE_DELTA_INGEST (default: false)`: flag to disable data ingestion, for example while initializing the sync
- `WAIT_FOR_INITIAL_SYNC (default: false)`: flag to not wait for initial ingestion (meant for debugging)
- `BYPASS_MU_AUTH_FOR_EXPENSIVE_QUERIES (default: false)`: (see code where it is called) This has repercussions you should know of!
- `DIRECT_DATABASE_ENDPOINT (default: http://virtuoso:8890/sparql)`: only used when BYPASS_MU_AUTH_FOR_EXPENSIVE_QUERIES is set to true
- `KEEP_DELTA_FILES (default: false)`: if you want to keep the downloaded delta-files (ease of troubleshooting)
### Model

#### Used prefixes

| Prefix | URI                                                       |
|--------|-----------------------------------------------------------|
| dct    | http://purl.org/dc/terms/                                 |
| adms   | http://www.w3.org/ns/adms#                                |
| ext    | http://mu.semte.ch/vocabularies/ext                       |

#### Sync task

##### Class

`ext:SyncTask`

##### Properties

| Name       | Predicate        | Range           | Definition                                                                                                                                   |
|------------|------------------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| status     | `adms:status`    | `adms:Status`   | Status of the sync task, initially set to `<http://lblod.data.gift/gelinkt-notuleren-mandatarissen-consumer-sync-task-statuses/not-started>` |
| created    | `dct:created`    | `xsd:dateTime`  | Datetime of creation of the task                                                                                                             |
| creator    | `dct:creator`    | `rdfs:Resource` | Creator of the task, in this case the mandatendatabank-consumer `<http://lblod.data.gift/services/gelinkt-notuleren-mandatarissen-consumer>` |
| deltaUntil | `ext:deltaUntil` | `xsd:dateTime`  | Datetime of the latest successfully ingested sync file as part of the task execution                                                         |

#### Sync task statuses

The status of the sync task will be updated to reflect the progress of the task. The following statuses are known:

* http://lblod.data.gift/gelinkt-notuleren-mandatarissen-consumer-sync-task-statuses/not-started
* http://lblod.data.gift/gelinkt-notuleren-mandatarissen-consumer-sync-task-statuses/ongoing
* http://lblod.data.gift/gelinkt-notuleren-mandatarissen-consumer-sync-task-statuses/success
* http://lblod.data.gift/gelinkt-notuleren-mandatarissen-consumer-sync-task-statuses/failure

### Data flow

At regular intervals, the service will schedule a sync task. Execution of a task consists of the following steps:

1. Retrieve the timestamp to start the sync from
2. Query the producer service for all diff files since that specific timestamp
3. Download the content of each diff file
4. Process each diff file in order

During the processing of a diff file, the insert and delete changesets are processed

**Delete changeset**
Apply a delete query triple per triple across all graphs

**Insert changeset**
Ingest the changeset in the graph `INGEST_GRAPH`.


If one file fails to be ingested, the remaining files in the queue are blocked since the files must always be handled in
order.

The service makes 2 core assumptions that must be respected at all times:

1. At any moment we know that the latest `ext:deltaUntil` timestamp on a task, either in failed/ongoing/success state,
   reflects the timestamp of the latest delta file that has been completly and successfully consumed
2. Maximum 1 sync task is running at any moment in time
